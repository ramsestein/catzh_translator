# Sistema Universal de Entrenamiento para Traducci√≥n Catal√°n-Chino

## üìã Descripci√≥n

Sistema modular y escalable para entrenar m√∫ltiples modelos de traducci√≥n autom√°tica entre catal√°n y chino. Dise√±ado para ejecutarse en estaciones de trabajo con GPUs potentes (A600, A100, etc.) y gestionar autom√°ticamente el entrenamiento de m√∫ltiples combinaciones modelo-dataset, con recuperaci√≥n autom√°tica y monitorizaci√≥n avanzada.

### üåü Caracter√≠sticas Principales

- **ü§ñ 6 Modelos Preconfigurados:** mT5 (Large/XL), NLLB-200, MADLAD-400, M2M-100, Llama 3.2
- **üìä Gesti√≥n Multi-Dataset:** Entrenamiento automatizado con m√∫ltiples datasets
- **üíæ Recuperaci√≥n Autom√°tica:** Contin√∫a desde checkpoints tras interrupciones
- **üîß Configuraci√≥n Centralizada:** F√°cil a√±adir nuevos modelos en `model_configs.py`
- **üìà Monitoreo en Tiempo Real:** Logs detallados y m√©tricas, resumen autom√°tico de experimentos
- **üöÄ Optimizado para GPUs Modernas:** Soporte para BF16, TF32, gradient checkpointing, LoRA

### ‚ö†Ô∏è Aviso Importante

**La primera vez que se ejecuta cada modelo, el sistema descargar√° los pesos (400MB a 3.7GB). Esta descarga puede tardar minutos y s√≥lo ocurre una vez por modelo (cach√© local).**

---

## üõ†Ô∏è Requisitos

### Hardware

- **GPU:** NVIDIA A600/A100 (recomendado) o RTX 4060+ (m√≠nimo)
- **VRAM:**
  - M√≠nimo: 8GB (modelos peque√±os: M2M-100, NLLB-200)
  - Recomendado: 24GB+
  - √ìptimo: 48GB+ (A600/A100)
- **RAM:** 32GB+
- **Almacenamiento:** 500GB+ SSD
- **Internet:** Necesario para descargar modelos la primera vez

### Software

- Python 3.8+
- CUDA 11.8+
- Ubuntu 20.04+ o WSL2

---

## üì¶ Instalaci√≥n

1. **Clonar el repositorio:**

   ```bash
   git clone https://github.com/ramsestein/catalan-chinese-translator.git
   cd catalan-chinese-translator
   ```

2. **Crear entorno virtual:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instalar dependencias:**

   ```bash
   pip install -r requeriments.txt
   ```

4. **Verificar instalaci√≥n:**

   ```bash
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"
   ```

---

## üìÅ Estructura del Proyecto

```
catalan-chinese-translator/
‚îú‚îÄ‚îÄ train_all.py              # Script principal de entrenamiento
‚îú‚îÄ‚îÄ universal_trainer.py      # L√≥gica principal del entrenamiento universal
‚îú‚îÄ‚îÄ model_configs.py          # Configuraci√≥n de modelos (a√±ade aqu√≠ nuevos modelos)
‚îú‚îÄ‚îÄ dataset_utils.py          # Utilidades y loader de datasets
‚îú‚îÄ‚îÄ trainer_base.py           # Base para entrenadores y m√©tricas
‚îú‚îÄ‚îÄ checkpoint_manager.py     # Gesti√≥n de checkpoints y recuperaci√≥n
‚îú‚îÄ‚îÄ run_training.sh           # Script bash para lanzar experimentos f√°cilmente
‚îú‚îÄ‚îÄ requeriments.txt          # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md                 # Este archivo
‚îÇ
‚îú‚îÄ‚îÄ datasets/                 # Datasets de entrenamiento (en repositorio como .zip)
‚îÇ   ‚îú‚îÄ‚îÄ dataset_optimizado.csv    # 2.5M pares de frases, curado
‚îÇ   ‚îî‚îÄ‚îÄ dataset_limpio.csv        # 5M pares de frases, filtrado
‚îÇ
‚îú‚îÄ‚îÄ training_outputs/         # Salida de modelos entrenados (se crea autom√°ticamente)
‚îÇ   ‚îî‚îÄ‚îÄ {modelo}_{dataset}/
‚îÇ       ‚îú‚îÄ‚îÄ checkpoint-*/
‚îÇ       ‚îú‚îÄ‚îÄ modelo_final/
‚îÇ       ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îî‚îÄ‚îÄ logs/                     # Logs detallados de entrenamiento
```

---

## üìä Datasets

Se encuentran en la carpeta datasets en formato .zip, deben descomprimirse para su uso

### Datasets Disponibles

| Dataset                 | Tama√±o | Descripci√≥n                    | Calidad |
| ----------------------- | ------ | ------------------------------ | ------- |
| dataset\_optimizado.csv | 2.5M   | Dataset curado de alta calidad | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê   |
| dataset\_limpio.csv     | 5M     | Dataset limpio y filtrado      | ‚≠ê‚≠ê‚≠ê‚≠ê    |

#### Formato de los Datasets

CSV con columnas `catalan` y `chino`:

```csv
catalan,chino
"Bon dia, com est√†s?","Êó©‰∏äÂ•ΩÔºå‰Ω†Â•ΩÂêóÔºü"
"M'agrada molt aprendre idiomes","ÊàëÂæàÂñúÊ¨¢Â≠¶‰π†ËØ≠Ë®Ä"
"Catalunya √©s un pa√≠s meravell√≥s","Âä†Ê≥∞ÁΩóÂ∞º‰∫öÊòØ‰∏Ä‰∏™ÁæéÂ¶ôÁöÑÂõΩÂÆ∂"
```

#### Preparar los Datasets

```bash
# Crear directorio datasets si no existe
mkdir -p datasets

# Mover tus archivos CSV al directorio datasets
mv dataset_optimizado.csv datasets/
mv dataset_limpio.csv datasets/
```

---

## ‚öôÔ∏è Configuraci√≥n

### Modelos Disponibles (editable en `model_configs.py`)

| Modelo     | ID         | Par√°metros | VRAM Min | Batch Size | Descarga (aprox.) |
| ---------- | ---------- | ---------- | -------- | ---------- | ----------------- |
| M2M-100    | m2m-100    | 418M       | 8GB      | 16         | 5 min             |
| NLLB-200   | nllb-200   | 600M       | 8GB      | 16         | 7 min             |
| mT5-Large  | mt5-large  | 1.2B       | 16GB     | 4-8        | 10 min            |
| MADLAD-400 | madlad-400 | 3B         | 24GB     | 4-8        | 20 min            |
| mT5-XL     | mt5-xl     | 3.7B       | 40GB     | 2-4        | 25 min            |
| Llama 3.2  | llama-3.2  | 3B         | 24GB     | 2-4        | 20 min            |

**Nota:** Los tiempos de descarga dependen de la conexi√≥n y s√≥lo ocurren la primera vez.

#### Agregar un Nuevo Modelo

Edita `model_configs.py` siguiendo la plantilla existente:

```python
MODEL_CONFIGS = {
    "nuevo-modelo": ModelConfig(
        name="Mi Nuevo Modelo",
        model_id="organizacion/modelo-id",
        model_type="seq2seq",  # o "causal"
        tokenizer_class="AutoTokenizer",
        model_class="AutoModelForSeq2SeqLM",
        batch_size=8,
        gradient_accumulation=4,
        learning_rate=3e-5,
        max_length=256,
        torch_dtype="float32",
        device_map="cuda"
    ),
    # ...otros modelos
}
```

---

## üöÄ Uso

### Usar el script Bash (`run_training.sh`)

**Opciones principales:**

- `-a`, `--all`: Entrenar todos los modelos con todos los datasets (por defecto)
- `-m`, `--models`: Entrenar modelos espec√≠ficos (`-m mt5-large nllb-200`)
- `-d`, `--datasets`: Usar datasets espec√≠ficos (`-d dataset_limpio.csv`)
- `-s`, `--single`: Prueba r√°pida (primer modelo + primer dataset)
- `-h`, `--help`: Ayuda

#### Ejemplos de uso

```bash
# Prueba r√°pida con M2M-100 (recomendado para empezar)
./run_training.sh --single

# Entrenar solo M2M-100
./run_training.sh --models m2m-100

# Entrenar varios modelos
./run_training.sh -m mt5-large nllb-200 m2m-100

# Entrenar todos los modelos con dataset optimizado
./run_training.sh --datasets datasets/dataset_optimizado.csv

# Entrenar todos los modelos y todos los datasets
./run_training.sh
```

### Uso directo en Python

```bash
# Modo single (prueba r√°pida)
python train_all.py --single

# Modelos espec√≠ficos
python train_all.py --models m2m-100 nllb-200

# Saltar verificaci√≥n de entorno
python train_all.py --skip-verification
```

### Ejecuci√≥n en Background

```bash
# Con nohup
nohup ./run_training.sh -m mt5-large > training.log 2>&1 &

# Con screen
screen -S training
./run_training.sh --models m2m-100 nllb-200
# Ctrl+A, D para detach

# Con tmux
tmux new -s training
./run_training.sh
# Ctrl+B, D para detach
```

---

## ‚è±Ô∏è Qu√© Esperar Durante el Entrenamiento

**Primera ejecuci√≥n (con descarga):**

```
üöÄ ENTRENANDO: M2M-100
üìä Dataset: dataset_optimizado
üìÅ Directorio: ./training_outputs/facebook_m2m100_418M_dataset_optimizado

üì• Cargando tokenizer M2M100Tokenizer...
üì• Cargando modelo M2M-100...
‚ö†Ô∏è  Primera vez: Descargando ~418MB - puede tardar varios minutos...
...
‚úÖ Modelo cargado
üìÇ Cargando datos desde: ./datasets/dataset_optimizado.csv
‚úÖ Datos limpios: 2449584/2466428 (99.3%)
üìà Divisi√≥n: Train=2204625, Val=122479, Test=122480
...
üèÉ Iniciando entrenamiento...
‚è≥ Preparando modelo y optimizador (puede tardar 1-2 minutos)...
üÜï Entrenando desde cero
```

**Ejecuciones posteriores** ser√°n mucho m√°s r√°pidas (modelo ya descargado y compilado).

---

## ‚è±Ô∏è Tiempos Estimados de Entrenamiento

En NVIDIA A600 (48GB VRAM):

| Dataset             | Modelo     | 1 √âpoca | 3 √âpocas | Total Estimado |
| ------------------- | ---------- | ------- | -------- | -------------- |
| dataset\_optimizado | M2M-100    | 2-3 h   | 6-9 h    | 7-10 h         |
|                     | NLLB-200   | 3-4 h   | 9-12 h   | 10-14 h        |
|                     | mT5-Large  | 5-7 h   | 15-21 h  | 18-24 h        |
|                     | MADLAD-400 | 8-10 h  | 24-30 h  | 26-32 h        |
|                     | mT5-XL     | 12-15 h | 36-45 h  | 40-48 h        |
|                     | Llama 3.2  | 10-12 h | 30-36 h  | 32-38 h        |
| dataset\_limpio     | M2M-100    | 4-6 h   | 12-18 h  | 14-20 h        |
| ...                 | ...        | ...     | ...      | ...            |

**Total estimado (entrenar todo): \~18-23 d√≠as**

---

## üìä Monitoreo

- **√öltimo log:**
  ```bash
  tail -f logs/training_*.log
  ```
- **Ver m√©tricas:**
  ```bash
  grep "loss" logs/training_*.log | tail -20
  ```
- **Uso de GPU:**
  ```bash
  watch -n 1 nvidia-smi
  nvidia-smi dmon -s pucvmet
  ```
- **Ver checkpoints guardados:**
  ```bash
  find training_outputs -name "checkpoint-*" -type d
  cat training_outputs/*/training_info.json
  ```

---

## üîß Soluci√≥n de Problemas

- **Entrenamiento lento al inicio:**
  - Normal: primera vez descarga pesos, compila modelo y optimizador.
- **Error: CUDA out of memory:**
  - Reduce batch\_size en `model_configs.py`.
  - Aumenta gradient\_accumulation.
  - Reduce max\_length.
  - Cierra otras aplicaciones que usen GPU.
- **Warnings tokenizer:**
  - Warnings sobre as\_target\_tokenizer y max\_length son normales.
- **Entrenamiento interrumpido:**
  - El sistema recupera autom√°ticamente desde el √∫ltimo checkpoint.

---

## üìà Resultados Esperados

Durante el entrenamiento se monitoriza `loss` y otras m√©tricas. El loss debe bajar de \~5-6 a \~1-2.

Estructura t√≠pica de salida:

```
training_outputs/
‚îú‚îÄ‚îÄ facebook_m2m100_418M_dataset_optimizado/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-500/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-1000/
‚îÇ   ‚îú‚îÄ‚îÄ modelo_final/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sentencepiece.bpe.model
‚îÇ   ‚îú‚îÄ‚îÄ training_info.json
‚îÇ   ‚îî‚îÄ‚îÄ test_metrics.json
‚îî‚îÄ‚îÄ ...
```

---

## üìã Archivo requeriments.txt

```
torch>=2.0.0
transformers>=4.35.0
accelerate>=0.24.0
sentencepiece>=0.1.99
protobuf>=3.20.0
pandas>=2.0.0
scikit-learn>=1.3.0
numpy>=1.24.0
peft>=0.5.0
tensorboard>=2.13.0
bitsandbytes>=0.41.0
datasets>=2.14.0
```

---

## ü§ù Contribuciones

- **Agregar un nuevo modelo:**
  - A√±adir configuraci√≥n en `model_configs.py`
  - Probar primero con `--single`
  - Verificar uso de memoria antes de escalar
  - Crear PR incluyendo resultados y logs
- **Reportar problemas:**
  - Incluye logs completos, especificaciones del sistema y el comando exacto usado.

---

## üìÑ Licencia

Este proyecto est√° bajo licencia MIT. Ver archivo LICENSE para m√°s detalles.

---
